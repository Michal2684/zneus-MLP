model:
  input_dim: 48
  output_dim: 1
  hidden_layers: [128, 64, 32]
  dropout: 0.1
  use_bn: False

training:
  batch_size: 64
  lr: 0.0003
  epochs: 200
  wandb_project: "MLP-project"  
  early_stop_patience: 10
  weight_decay: 0.0001

